---
title: "LLM Evaluation Inconsistencies Research"
excerpt: "Research investigating inconsistencies in Large Language Model evaluation methodologies.<br/><img src='/images/portfolio-placeholder.png'>"
collection: portfolio
---

This research project investigates critical inconsistencies in how Large Language Models are evaluated in multiple-choice question answering tasks. The work reveals fundamental issues in current evaluation methodologies and proposes improvements for more reliable model assessment.

**Key Contributions:**
- Identification of evaluation inconsistencies
- Analysis of scoring mechanisms
- Proposed improvements to evaluation protocols
- Comprehensive benchmarking study

**Publication:** ACL 2025

**Impact:**
This work has important implications for the development and assessment of Large Language Models, helping the community establish more robust evaluation standards.
