---
title: "MCQA-Eval"
excerpt: "Right Answer, Wrong Score - Evaluation framework for uncovering inconsistencies in LLM evaluation for multiple-choice question answering. This work reveals critical issues in current evaluation methodologies. Published at ACL 2025 (Findings)."
collection: portfolio
date: 2025-01-01
permalink: /portfolio/mcqa-eval
paperurl: 'https://github.com/SapienzaNLP/mcqa-eval'
---

MCQA-Eval is an evaluation framework for analyzing inconsistencies in how Large Language Models are evaluated in multiple-choice question answering tasks. The framework reveals critical issues in current evaluation methodologies and provides tools for more reliable model assessment.

**Publication:** ACL 2025 (Findings)

**Paper:** [https://aclanthology.org/2025.findings-acl.950/](https://aclanthology.org/2025.findings-acl.950/)

**GitHub:** [https://github.com/SapienzaNLP/mcqa-eval](https://github.com/SapienzaNLP/mcqa-eval)
